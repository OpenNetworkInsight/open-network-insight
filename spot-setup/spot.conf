#spot.conf should always be located in /etc/spot.conf
#file formated as standard .INI

[DEFAULT]
#base user and data source config
#HDFS
HUSER=/user/spot
DSOURCES=flow dns proxy
DFOLDERS=binary csv hive stage
HPATH=%(HUSER)s/%(TYPE)s/scored_results/%(FDATE)s
#local fs
LUSER=/home/spot
LPATH=%(LUSER)s/ml/%(TYPE)s/%(FDATE)s
RPATH=%(LUSER)s/ipython/user/%(FDATE)s
LDAPATH=%(LUSER)s/ml/oni-lda-c
LIPATH=%(LUSER)s/ingest
#Spot Components
UINODE=node03
MLNODE=node04
GWNODE=node16
#list of Worker nodes, excluding MLNODE
NODES=
    node-01
    node-02
    node-03
    
#Default values affecting Machine Learning Results
MAXRESULTS=3000
TOL=1e-6
DUPFACTOR=1000
TOPIC_COUNT=20

[database]
IMPALA_DEM=node04
DBNAME=spot

[kerberos]
KRB_AUTH=false
KINITPATH=
KINITOPTS=
KEYTABPATH=
KRB_USER=


#Set SPK_CONFIG to True to use "Optional Values" in spark-submit command
#for more information see ./spot-ml/SPARKCONF.md

[spark]
#default options
SPK_EXEC=400
SPK_EXEC_MEM=2048m
#Optional Values
SPK_CONFIG=
SPK_DRIVER_MEM=
SPK_DRIVER_MAX_RESULTS=
SPK_EXEC_CORES=
SPK_DRIVER_MEM_OVERHEAD=
SPK_EXEC_MEM_OVERHEAD=

[mpi]
#command to run MPI
MPI_CMD=mpiexec
#command to prepare system for MPI, eg. load environment variables
MPI_PREP_CMD=
#number of processes to run in MPI
PROCESS_COUNT=20

[ingest]
hdfs_app_path=hdfs application path
collector_processes=5
ingestion_interval=1
kafka_server=kafka ip
kafka_port=9183
zookeeper_server=localhost
zookeeper_port=2181
message_size=999999
# spark streaming options
driver_memory=
spark_exec=
spark_executor_memory=
spark_executor_cores=
spark_batch_size=

[flow]
type=flow
collector_path=/path_to_flow_collector
local_staging=/tmp/
process_opt=""
FLOW_PATH=%(HUSER)s/%(TYPE)s/hive/y=%(YR)s/m=%(MH)s/d=%(DY)s/
supported_files=nfcapd.

[dns]
type=dns
collector_path=/path_to_dns_collector
local_staging=/tmp/
pcap_split_staging=/tmp/
process_opt="-E separator=, -E header=y -E occurrence=f -T fields -e frame.time -e frame.time_epoch -e frame.len -e ip.src -e ip.dst -e dns.resp.name -e dns.resp.type -e dns.resp.class -e dns.flags.rcode -e dns.a 'dns.flags.response == 1'"
DNS_PATH=%(HUSER)s/%(TYPE)s/hive/y=%(YR)s/m=%(MH)s/d=%(DY)s/
supported_files=.pcap

[proxy]
type=proxy
collector_path=/path_to_proxy_collector
supported_files=["log"]
parser=bro_parser.py
PROXY_PATH=%(HUSER)s/%(TYPE)s/hive/y=%(YR)s/m=%(MH)s/d=%(DY)s/
supported_files=.log
